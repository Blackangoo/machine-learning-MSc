#+TITLE: The perceptron algorithm
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\theta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{\Theta}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3

* The Perceptron
*** The perceptron algorithm
**** Input
- Feature space $X \subset \Reals^n$.
- Label space $Y = \{-1, 1\}$.
- Data $(x_t, y_t)$, $t \in [T]$,  with $x_t \in X, y_t in Y$.
**** Algorithm
- $w_1 = w_0$.

- For $t = 1, \ldots, T$.
-- $a_t = \sgn(w_t^\top x_t)$.
-- If $a_t \neq y_t$
--- $w_{t+1} = w_t + y_t x_t$
-- Else
--- $w_{t+1} = $w_t$
- Return $w_{T+1}$
*** The Perceptron Theorem
 The number of mistakes made by the perceptron algorithm is bounded by
 $(r/\rho)^2$, where $\|x_t\|\leq r$, $\rho \leq y_t (v^\top x_t) /
 \|v\|$ for some *margin* $\rho$ and *hyperplane* $v$.
	 
*** Perceptron examples
**** Example 1: One-dimensional data
- Done on the board
- Shows how the algorithm works.
- Demonstrates the idea of a margin

**** Example 2: Two-dimensional data
- See [[file:src/NeuralNetworks/perceptron.py][in-class programming exercise]]

*** Why doesn't the perceptron always work?
* Gradient methods
** Gradients for optimisation
*** Gradient methods for expected value :example:
**** Estimate the expected value
$x_t \sim P$ with $\E_P[x_t] = \mu$.
**** Objective
\[
\min_\param \E_P[(x_t - \param)^2].
\]
**** Derivative
Idea: at the minimum the derivative should be zero.
\[
d/d\param \E_P[(x_t - \param)^2]
= \E_P[d/d\param(x_t - \param)^2]
= \E_P[-(x_t - \param)]
= \E_P[x_t] - \param.
\]

Setting the derivative to 0, we have $\param = \E_P[x_t]$. This is a simple solution.
**** Real-world setting
- The objective function does not result in a simple solution
- The distribution $P$ is not known.
- We can sample $x \sim P$.

**** Stochastic gradient for mean estimation
\begin{align*}
 \frac{d}{d\param} \E_P [(x - \param)^2] 
&= \int_{-\infty}^\infty dP(x) \frac{d}{d\param} (x - \param)^2
\\
&=  \frac{d}{d\param} \int_{-\infty}^\infty dP(x) (x - \param)^2
\end{align*}

** The perceptron as a gradient algorithm
*** The classification error cost function

*** The entropy cost function


