#+TITLE: Multi-Layer Perceptrons and Deep Learning
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\theta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{\Theta}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3

* Features and layers
*** Layering and features
**** Fixed layers
- Fixed number of units, architecture and parameters
- Example 1: Feature transformation
- Example 2: Softmax layer

**** Adaptive layers
- Fixed units and architecture
- Adaptive parameters
- Example 1: Linear layer
- Example 2: Convolutional layers (e.g. images)

*** Softmax layer
- Features $x$
- Linear activation layer
\[
z_i = \vparam_i^\top x
\]

**** Softmax output layer
We want to translate the real-valued $z_i$ into probabilities:
\[
y_i = \frac{\exp(z_i)}{\sum_j \exp(z_j)}.
\]

*** Random projections
- Features $x$
- Hidden layer activation $z$
- Output $y$

**** Hidden layer: Random projection
Here we project the input into a high-dimensional space
\[
z_i = \sgn(\vparam_i^\top x),
\]
where $\mparam = [\vparam_i]_{i=1}^m$.

**** The reason for random projections
- The high dimension makes it easier to learn.
- The randomness ensures we are not learning something spurious.

* Algorithms
*** Back-propagation
**** The chain rule
\[
f : X \to Z, \qquad g : Z \to Y,
\qquad \frac{dg}{dx} = \frac{dg}{df} \frac{df}{dx}
\]

**** Parametrised functions
\begin{align}
f: \mathcal{W} \times X \to Z, && g: \Omega \times Z \to Y, &&\pi = fg \tag{network mappings}
\\
\ell(D, \pi) = \sum_{(x,y) \in D} [y - \pi(x)]^2
\end{align}
**** Gradient descent with /back-propagation/
Apply the chain rule 
\[
\nabla_{w, \omega} \pi = \nabla_\omega
\]

* Network Architectures
*** Neural architectures
**** Layers
- Input to layer $x \in R^n$ 
- Output from layer $z \in R^m$.

**** Linear layer
Transform the output of previous layers or features into either:
- A higher-dimensional space.
- A lower-dimensional space.
- They have adaptive parameters.
- Parameters can be dependent on each other for invariance (cf. convolution)

**** Non-linear layers
- Simple transformations of previous output
- Examples: Sigmoid, Softmax

** Common type of Layers
*** Linear layer
**** Definition
This is a linear combination of inputs $x \in \Reals^n$ and parameter matrix $\bW \in \Reals^{m \times n}$
where $\bW = \begin{bmatrix}
	\vectorsym{w}_1\\
        \vdots\\
	\wi\\
	\vdots\\
	\vectorsym{w}_m
\end{bmatrix}
=
\begin{bmatrix}
w_{1,1} & \cdots & w_{1,j} & \cdots & w_{1,m}\\
\vdots  & \ddots & \vdots  & \ddots & \cdots \\
w_{i,1} & \cdots & w_{i,j} & \cdots & w_{i,m}\\
\vdots  & \ddots & \ddots  & \ddots & \cdots \\ 	   
w_{n,1} & \cdots & w_{i,j} & \cdots & w_{n,m}
\end{bmatrix}$

\[
f(\bW, \bx) = \bW \bx 
\qquad
f_i(\bW, \bx)= \wi \cdot \bx =  \sum_{j=1}^n w_{i,j} x_i,
\]


**** Gradient 
Each partial derivative is simple:
\[
\frac{\partial}{\partial \wij} f_k(\bW, x) = x_i \ind{j = k}
\]

*** Sigmoid layer
**** Definition
This layer transforms each input non-linearly
\[
f_j(\bx) 1/[1 + \exp(-x_j)] =
\]
without looking at the other inputs.

**** Derivative
So let us ignore the other inputs for simplicity:
\[
\frac{d}{dx} f(x) = \exp(-x)/[1+\exp(-x)]^{2}
\]

*** Softmax layer

