#+TITLE: Generative Modelling
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\theta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{\Theta}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3

** Classification: Generative modelling
   #+TOC: headlines [currentsection,hideothersubsections]
*** Generative modelling
**** general idea
- Data $(x,y)$.
- Need to model $P(y | x)$.
- Model the complet data distribution: $P(x | y)$, $P(x)$, $P(y)$.
- Calculate \(  P(y | x) = \frac{P(x | y) P(x)}{P(y)}. \)
**** Examples
- Naive Bayes classifier
- Gaussian Mixture Classifier
**** Modelling the data distribution
- Need to estimate the density $P(x | y)$ for each class $y$.

*** Classification: Naive Bayes Classifier
- Data $(x,y)$
- $x \in X$
- $y \in Y \subset \mathbb{N}$, $N_i$: amount of data from class $i$

  
**** Separately model each class
- Assume each class data comes from a different normal distribution
- $x | y = i \sim \Normal(\mu_i, \sigma_i I)$
- For each class, calculate
  - Empirical mean $\hat{\mu}_i = \sum_{t : y_t = i} x_t / N_i$
  - Empirical variance $\hat{\sigma}_i$.

**** Decision rule
Use Bayes's theorem:
\[
P(y | x) = P(x | y) P(y) / P(x),
\]
choosing the $y$ with largest posterior $P(y | x)$.
- $P(x | y = i) \propto \exp(- \|\hat{\mu}_i - x\|^2/\hat{\sigma}_i^2$
** Density estimation
*** General idea
**** Parametric models
- Fixed histograms
- Gaussian Mixtures
**** Non-parametric models
- Variable-bin histograms
- Infinite Gaussian Mixture Model
- Kernel methods

*** Histograms
**** Fixed histogram
- Hyper-Parameters: number of bins
- Parameters: Number of points in each bin.
**** Variable histogram
- Hyper-parameters: Rule for constructing bins
- Generally $\sqrt{n}$ points in each bin.

*** Gaussian Mixture Model
**** Hyperparameters:
- Number of Gaussian $k$.
**** Parameters:
- Multinomial distribution $\vparam$ over Gaussians
- For each Gaussian $i$, center $\mu_i$, covariance matrix $\Sigma_i$.
**** Model. For each point $x_t$:
- $c_t = i$ w.p. $\theta_i$
- $x_t | c_t = i \sim \Normal(\mu_i, \Sigma_i)$.
**** Algorithms:
- Expectation Maximisation
- Gradient Ascent
- Variational Bayesian Inference (with appropriate prior)

*** GMM with EM
**** Objective function: log-likelihood
\[
\ln P(x | \theta, \mu, \Sigma) = \ln \sum_i \theta_i P(x | \mu_i, \sigma_i)
\]

**** Expectation Step

**** Maximization Step

*** Minorise-Maximise
- $f(\vparam)$: Target function to *maximise*
- $Q(\vparam | \vparam^{(k)})$: surrogate function

**** $Q$ Minorizes $f$
This means surrogate is always a lower bound so that
\[
f(\vparam) \geq Q(\vparam | \vparam^{(k)}),
\qquad
f(\vparam^{(k)}) \geq Q(\vparam^{(k)} | \vparam^{(k)}),
\]

**** Algorithm
- Calculate: $Q(\vparam | \vparam^{(k)}$
- Optimise: $\vparam^{(k+1)} = \argmax_\vparam Q(\vparam | \vparam^{(k)}$.

*** Expectation Maximisation     

**** EM Algorithm
- Initial parameter $\vparam^{(0)}$, observed data $D$
- For $k=0, 1, \ldots$
-- Expectation step:
\[
Q(\vparam | \vparam^{(k)})
 \defn \E_{Z \sim P(Z | D, \vparam^{(k)})} [\ln P(D, Z | \vparam) ]
 = \int_{\mathcal{Z}} [\ln P(D, Z | \vparam)] d P(Z  \mid D, \vparam^{(k)})
\]
-- Maximisation step:
\[
\vparam^{(k+1)} = \argmax_\vparam Q(\vparam, \vparam^{(k)}).
\]
**** Analysis 
(see wiipedia)

*** GMM Classifier :exercise:
**** Base class: sklearn GaussianMixtureModel
- /fit()/ only works for Density Estimaiton
- /predict()/ only predicts cluster labels
**** Problem
- Create a GMMClassifier class
- /fit()/ should take X, y, arguments
- /predict()/ should predict class labels
- Hint: Use /predict_proba()/ and multiple GMM models

