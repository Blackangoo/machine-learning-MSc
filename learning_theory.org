#+TITLE: Support Vector Machines
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \bz {\vectorsym{z}}
#+LaTeX_HEADER: \newcommand \param {\beta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\beta}}
#+LaTeX_HEADER: \newcommand \Params {\mathcal{B}}
#+LaTeX_HEADER: \newcommand \vmult {\vectorsym{\lambda}}
#+LaTeX_HEADER: \newcommand \mult {\lambda}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3

* Basic inequalities
* Regularization
* Generalisation error and SVMs
** Learning theory
*** Learning theory theorem
Let $R$ be our risk function (e.g. classification error). We would like
the empirical risk to converge to the minimum risk.

** The VC dimension
*** The VC Dimension
- A family of classifiers $F = \{f_\vparam(x) : \vparam \in \Params\}$, $f_\vparam : \to \{-1, 1\}$.
**** The VC dimension of $F$
The largest number of points that can be shattered by members of $F$.

**** Generalisation error
The error of a class bounded by
\[
\epsilon(D_T) + O\left(\sqrt{1/T VC(F) \ln(T/\delta)}\right)
\]
w.p. $1 - \delta$, where $\epsilon(D_T)$ is the training data error.


*** Finite $F$ Class
**** Restriction
The restriction of $F$ to a set of points $S$ in $X$ is $\{0,1\}^{|F|}$ and given by $F_S = \{f

**** VC Dimension of finite $F$
If $F$ is finite, then $VC(F) \leq \log_2 (|F|)$

