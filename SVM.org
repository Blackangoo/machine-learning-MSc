#+TITLE: Support Vector Machines
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \param {\beta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\beta}}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
* Background
** Maximal margin
*** Hyperplane
If $x \in \Reals^n$, then an affine subspace of dimension $n-1$ is a hyperplane.
**** Definition
A hyperplane in $\Reals^n$ is the set of points satisfying
\[
\{\bx : \param_0 + \vparam^\top \bx =0 \}
\]
**** Separating hyperplane
Consider a dataset $(\bx_t, y_t)$ of points with $y_t \in \{-1, 1\}$. If 
\[
(\param_0 + \vparam^\top \bx_t) y_t \qquad \forall t
\]
then the hyperplane separates the dataset.
*** The maximal margin hyperplane
**** The margin
For any a dataset $(\bx_t, y_t)$, and hyperplane we can define the margin
\[
M = \min_t (\param_0 + \vparam^\top \bx_t) y_t 
\]
as the minimum distance between the hyperplane and a correctly classied point.
**** The maximal margin hyperplane
Similarly, there is some $\param_0, \vparam$ that achieves the maximum separation:
\[
\max_{\param_0, \vparam} \min_t (\param_0 + \vparam^\top \bx_t) y_t 
\]
*** The maximum margin classifier
**** The optimisation problem
We can write the problem like this
\begin{align}
\max_{\param_0, \vparam, M} & M \tag{maximise the margin}\\
\textrm{s.t.} & \|\vparam\| = 1 \tag{invariance} \\
 & y_t(\param_0 + \vparam^\top \bx_t) \geq M && \forall t \in [T] \tag{margin for all examples}.
\end{align}
And we can divide by $\|\vparam\|$ to remove the norm constraint:
\[
 y_t(\param_0 + \vparam^\top \bx_t) \geq M \|\vparam\|, \qquad \forall t \in [T] 
\]
Setting $\|\vparam\| = 1/M$, we can rewrite this as
\begin{align*}
\min_{\param_0, \vparam} & \frac{1}{2}\|\vparam\|^2\\
\textrm{s.t.} &  y_t(\param_0 + \vparam^\top \bx_t) \geq 1 && \forall t 
\end{align*}
*** Quadratic programming
**** A quadratic program
This has the form:
\[\min_\vparam \|\vparam\|^2, \qquad \textrm{s.t.} \vparam^\top x_t \geq 1 \forall t.
\]
**** The lagrange method for constrained optimisation
*** Lagrange methods
**** Constrained optimisation
Consider $f : X \to \Reals$ and $h : X \to \Reals^n$
\[
\min_x f(x), \qquad \textrm{s.t.~} h(x) = 0
\]
**** Lagrange multipliers
For any local minimum $x^*$, there is a unique vector $\lambda^* \in \Reals^n$ so that
\[
\nabla f(x^*) + \sum_i \lambda^*_i \nabla h_i(x^*) = 0.
\]
**** The Langrangian function
We first augment the original cost function to the \alert{Langrangian}
\[
L(x, \lambda) = f(x) + \sum_{i=1}^n \lambda_h h_i(x)
\]
Then, for any local minimum $x^*$ it holds
\[
\nabla_x L(x^*, \lambda^*) = 0, \qquad \nabla_\lambda(x^*, \lambda^*) = 0.
\]
** Support Vector Machines
*** Support vector machines
- Hyperplanes do not always work
- How about a non-linear boundary?
- Instead of mapping the inputs through a non-linearity, map inner products to a kernel
**** Kernelised linear functions
We can rewrite
\[
f(\bx) = \param_0 + \sum_{i=1}^n \param_i x_i
\]
in terms of a kernel $K : X \times X \to \Reals$:
\[
f(\bx) = \param_0 + \sum_{t=1}^T \alpha_t K(\bx, \bx_t), \qquad K(\bx, \bx_t) = \bx^\top \bx_t
\]
because we can find $\vectorsym{\alpha}$ so that 
\[
\sum_i  \sum_t \left(\alpha_t x_{t,i}\right) x_i = \sum_i \param_i x_i.
\]
In fact it is sufficient to have: $\sum_t \alpha_t x_{t,i} = \param_i$.
*** Kernels
**** Radial Basis Function
A simple type of non-linear layer in neural networks:
\[
f(x) = \sum_i \alpha_i K(\bx,  \vectorsym{c}_i),
\qquad
K(\bx, \vectorsym{c}_i) = \exp(-\|\bx - \vectorsym{c}_i\|^2),
\]
where $\vectorsym{c}_t$ are \alert{fixed centroids}
**** Kernels in SVMs
Instead of fixed kernels, use the \alert{training data}:
\[
f(x) = \sum_t \alpha_t K(\bx,  \bx_t),
\]
**** Some common kernel choices
- Linear: $K(\bx, \bx') = \bx^\top \bx$.
- RBFs: $K(\bx, \bx') = \exp(-\|bx - \bx'\|^2)$
- Polynomial: $K(\bx, \bx') = (1 + \bx^\top \bx)^d$.
