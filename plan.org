#+TITLE: Machine Learning
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3

* The problems of Machine Learning (2 weeks)
** Models, hypotheses
*** Data collection
*** Models from data
*** k-Nearest-Neighbour :example:
*** Unsupervised learning
*** Supervised learning
*** Reinforcement learning
** Probability background
***  Probability facts
**** Axioms of probability
- $P(\Omega) = 1$
- If $A \cap B = \emptyset$ then $P(A \cup B) = P(A) + P(B)$.
- $P(\emptyset) = 0$.
**** Marginalisation
If $A_1, \ldots, A_n$ are a partition of $\Omega$
\[
P(B) = \sum_{i = 1}^n P(B \cap A_i).
\]

*** Expectation
For any random variable $f: \Omega \to \Reals$, the expectation with respect to a probability measure $P$ is
\[
\E_P(f) = \sum_{\omega \in \Omega} f(\omega) P(\omega).
\]
*** Conditional probability
The conditional probability of an event $A$ given an event $B$ is defined as 
\[
P(A | B) = \frac{P(A \cap B)}{P(B)}
\]
*** Conditional expectation
The conditional expectation of a random variable $f: \Omega \to \Reals$, with respect to a probability measure $P$ conditioned on some event $B$ is simply
\[
\E_P(f | B) = \sum_{\omega \in \Omega} f(\omega) P(\omega | B).
\]

*** Bayes's theorem
**** Reversing the dependency
\[
P(A | B) = \frac{P(B | A)}{P(B)} 
\]
**** The general case
If $A_1, \ldots, A_n$ are a partition of $\Omega$, meaning that they
are mutually exclusive events (i.e. $A_i \cap A_j = \emptyset$ for $i
\neq j$) such that one of them must be true (i.e. $\bigcup_{i=1}^n A_i =
\Omega$), then
\[
P(B) = \sum_{i=1}^n P(B | A_i) P(A_i)
\]
and 
\[
P(A_j | B) = \frac{P(B | A_j)}{\sum_{i=1}^n P(B | A_i) P(A_i)}
\]


* Learning as Optimisation (4 weeks)
** Objective functions
*** Supervised learning objectives
**** Classification
- Predict the labels correctly
- Have an appropriate confidence level
**** Regression
- Predict the mean correctly
- Have an appropriate variance around the mean
*** Unsupervised learning objectives
- Reconstruct the data well
- Be able to generate data
*** Reinforcement learning objectives
- Maximise total reward

** Linear neural networks
** Multi-layer neural networks
* Learning as Probabilistic Inference (4 weeks)
** Probabilistic Models
* Reinforcement Learning (2 weeks)

