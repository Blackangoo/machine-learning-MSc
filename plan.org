#+TITLE: Machine Learning and Data Mining
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3

* The problems of Machine Learning (2 weeks)
#+TOC: headlines [currentsection]
** Models, hypotheses
*** Data collection
- Why do we need data?
- What data do we need?
- How much data do we want?
- How will we collect the data?
*** Models from data
**** Prediction
- Will it rain tomorrow?
- How much will bitcoin be worth next year?

**** Inference
- Does my poker opponent have two aces?
- What is the mass of the moon?
- What is the law of gravitation?

**** Decision Making
- Should I go hiking tomorrow?
- Should I buy some bitcoins?
- Should I fold, call, or raise in my poker game?

*** Learning from data
**** Unsupervised learning
- Given data $x_1, \ldots, x_T$.
- How is the data generated?

**** Supervised learning
- Given data $(x_1, y_1), \ldots, (x_T, y_T)$
- Learn about the relationship between $x_t$ and $y_t$

**** Reinforcement learning
- No data is given: the agent acts to obtain data.
- Learn to act in an *unknown* world through interaction and reward
  feedback.



* Learning as Optimisation (4 weeks)
#+TOC: headlines [currentsection]
** Objective functions
*** Supervised learning objectives
**** Classification
- Predict the labels correctly
- Have an appropriate confidence level
**** Regression
- Predict the mean correctly
- Have an appropriate variance around the mean
*** Unsupervised learning objectives
- Reconstruct the data well
- Be able to generate data
*** Reinforcement learning objectives
- Maximise total reward

** $k$ Nearest Neighbours
*** The Nearest Neighbour algorithm
**** Pseudocode
- Input: Data $(x_t, y_t)_{t=1}^T$, test point $x$, distance $d$
- Find $t^* \in \argmin_{t} d(x_t, x)$:
  - $D = \infty$, $t^* = \emptyset$.
  - For $t \in 1, \ldots, T$:
    - If $d(x_t) < D$: $D = d(x_t), t^* = t$.
  - EndFor
- Return $y^* = y_{t^*}$

** Linear neural networks
*** Simple linear regression
**** Input and output
- Data pairs $(x_t, y_t)$, $t = 1, \ldots, T$.
- Input $x_t \in \Reals^n$
- Output $y_t \in \Reals$.

**** Parametrised function
- Parameters $\param \in \Reals^n$
- Function $f_\param : \Reals^n \to \Reals$, defined as
\[
f_\param(x_t) = \param^\top x_{t} = \sum_{i=1}^n \param_i x_{t,i}
\]

**** Optimisation goal: Miniminise mean-squared error.
\[
\min_\param \sum_t [y_t - f_\param(x_t)]^2
\]

** Multi-layer neural networks
* Learning as Probabilistic Inference (4 weeks)
** Probabilistic Models
#+TOC: headlines [currentsection]
* Reinforcement Learning (2 weeks)
#+TOC: headlines [currentsection]

